#!/usr/bin/env python3

# usage: getBNBData [target directory]
#
#        if the target directory is omitted, ./BNBDaten is used by default

import io
import os
import re
import sys
import time

from bs4 import BeautifulSoup
from urllib.parse import urljoin
import requests
import zipfile

# adjust if necessary
debug = False
maxretry = 5
datadir = './BNBDaten' if len(sys.argv) < 2 else sys.argv[1]
urlbase = 'https://www.bl.uk/collection-metadata/'
urlpath = '/new-bnb-records'

def request_wrapper(url):
    for retry in range(maxretry):
        r = requests.get(url)
        if r.status_code == 429:
            if debug:
                print(f"Got status code {r.status_code}, retry #{retry+1}")
            time.sleep(retry+1)
            continue
        elif r.status_code != 200:
            print(f"Status code {r.status_code} for {url}")
            break
        else:
            return r
    return None

def get_files(page):
    soup = BeautifulSoup(page.text, 'html.parser')
    lilist = [ i.parent for i in soup.find_all(string=re.compile("bnbrdf")) ]
    if len(lilist) == 0:
        return None
    ret = []
    for li in lilist:
        zipurl = li.find("a").get("href")
        if not zipurl.startswith('http'):
            zipurl = urljoin(urlbase,zipurl)
        rdffile = re.sub(r'.*(bnbrdf_n\d+)\.zip.*', r'\1.rdf', li.text, flags=re.I)
        ret.append((zipurl,rdffile))
    return ret

def main():
    # create datadir if necessary and get list of existing files
    if not os.path.exists(datadir):
        os.makedirs(datadir)
    locfiles = [ f.lower() for f in os.listdir(datadir) if f.lower().endswith('.rdf') ]

    pageurl = urlbase + urlpath

    # retrieve page
    if debug:
        print(f"Downloading summary page {pageurl}")
    page = request_wrapper(pageurl)
    if page is None:
        print(f"Error downloading {pageurl}")
        sys.exit(1)

    # extract file links
    if debug:
        print(f"Parsing result and extracting links")
    files = get_files(page)
    if files is None:
        print(f"Error getting file links from {pageurl}")
        sys.exit(1)

    # download files
    retcode = 0
    for zipurl, rdffile in files:
        if rdffile.lower() in locfiles:
            continue
        if debug:
            print(f"Downloading {rdffile} from {zipurl}")
        r = request_wrapper(zipurl)
        if r is None:
            print(f"Error downloading {rdffile}")
            retcode += 1
            continue
        if r.headers['content-type'] != 'application/x-zip-compressed':
            print(f"Warning: Unexpected content type {r.headers['content-type']} for {zipurl}")
            retcode += 1
            continue

        try:
            with zipfile.ZipFile(io.BytesIO(r.content)) as z:
                for f in z.infolist():
                    name, date_time = f.filename, f.date_time
                    name = os.path.join(datadir, name)
                    with open(name, 'wb') as outfile:
                        outfile.write(z.open(f).read())
                    date_time = time.mktime(date_time + (0,0,-1))
                    os.utime(name, (date_time, date_time))
        except Exception as e:
            print("Caught exception while unzipping:", e)
            retcode += 1

    # if any problems occured during download or extraction, retcode is > 0
    sys.exit(retcode)

if __name__ == "__main__":
    main()
